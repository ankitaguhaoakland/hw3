---
title: "HW5 - Problem 3 - Orange Juice classification"
author: "misken"
date: "March 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 3 - Predicting orange juice purchases

The dataset is available as part of the ISLR package. You can see the
documentation for that package or the following link describes the OJ
dataset - https://rdrr.io/cran/ISLR/man/OJ.html.

**SUGGESTION**: See the material available in Downloads_StatModels2 from the
session on classification models in R. In particular, the folder on
logistic regression and the example in the folder intro_class_HR/ will
be useful.

## Data prep

We'll do a little data prep to set things up so that we are trying to
predict whether or not the customer purchased Minute Maid (vs Citrus Hill.)
Just run the following chunks to load the dataset, do some data prep and
then partition the data into training and test sets.

```{r loaddata}
ojsales <- (ISLR::OJ)
```

Loading a few libraries
```{r}
library(dplyr)
library(ggplot2)
library(useful)
library(coefplot)
library(caret)
library(lattice)
#library("ROCR")
```

Clean up the storeid related fields. Drop Store7 field.

```{r}
ojsalesNoStoreID7 <- ojsales %>% filter(StoreID < 7) 
```

With the original dataframe
```{r factors}
ojsales$StoreID <- as.factor(ojsales$StoreID)

# Create a new variable to act as the response variable.
ojsales$MM <- as.factor(ifelse(ojsales$Purchase=="MM",1,0))
```

Now we'll just take a subset of the columns as there are a few that contain the
same information. Remember, the new column `MM` is the one we are trying to
predict.

```{r subset}
ojsales_subset <- ojsales[, c(19, 3:13, 15:17)]
```

Just run this chunk to create training and test datasets. This way we'll
all be working with the same datasets. Notice that the test set is 10% of
the full dataset.

```{r partition}
set.seed(167)
sample_size <- ceiling(0.10 * nrow(ojsales))
testrecs <- sample(nrow(ojsales_subset),sample_size)
ojsales_test <- ojsales_subset[testrecs,]
ojsales_train <- ojsales_subset[-testrecs,]  # Negative in front of vector means "not in"
rm(ojsales_subset, ojsales) # No sense keeping a copy of the entire dataset around
```

## Your job

With the dropped StoreID 7 dataframe and adding the Response variable MM within the dataframe 
```{r}
ojsalesNoStoreID7$StoreID <- as.factor(ojsalesNoStoreID7$StoreID)

# Create a new variable to act as the response variable.
ojsalesNoStoreID7$MM <- as.factor(ifelse(ojsalesNoStoreID7$Purchase=="MM",1,0))
```

Now we'll just take a subset of the columns as there are a few that contain the
same information. Remember, the new column `MM` is the one we are trying to
predict.

```{r}
ojsalesNoStoreID7_subset <- ojsalesNoStoreID7[, c(19, 3:13, 15:17)]
```

Just run this chunk to create training and test datasets. This way we'll
all be working with the same datasets. Notice that the test set is 10% of
the full dataset.


```{r}
set.seed(167)
sample_size <- ceiling(0.10 * nrow(ojsalesNoStoreID7))
testrecs <- sample(nrow(ojsalesNoStoreID7_subset),sample_size)
ojsalesNoStoreID7_test <- ojsalesNoStoreID7_subset[testrecs,]
ojsalesNoStoreID7_train <- ojsalesNoStoreID7_subset[-testrecs,]  # Negative in front of vector means "not in"
rm(ojsalesNoStoreID7_subset, ojsalesNoStoreID7) # No sense keeping a copy of the entire dataset around
```


# A Little EDA
```{r}
head(ojsales_train)
head(ojsalesNoStoreID7)
```
Let's check out our data frame
```{r}
str(ojsales_train)

```

Summary of our dataframe
```{r}
summary(ojsales_train)
```



```{r}
# ojsales_train$MM <- as.factor(ojsales_train$MM)
# ojsales_train$StoreID <- as.factor(ojsales_train$StoreID)
# ojsales_train$SpecialCH <- as.factor(ojsales_train$SpecialCH)
# ojsales_train$SpecialMM <- as.factor(ojsales_train$SpecialMM)
# ojsales_train$PriceCH <- as.factor(ojsales_train$PriceCH)
# ojsales_train$SalePriceCH <- as.factor(ojsales_train$SalePriceCH)
# ojsales_train$PriceMM <- as.factor(ojsales_train$PriceMM)
# ojsales_train$SalePriceMM <- as.factor(ojsales_train$SalePriceMM)
# 
# HR_test$promotion_last_5years <- as.factor(HR_test$promotion_last_5years)
# HR_test$Work_accident <- as.factor(HR_test$Work_accident)
# HR_test$sales <- as.factor(HR_test$sales)
# HR_test$salary <- as.factor(HR_test$salary)
# HR_test$left <- as.factor(HR_test$left)
```



Some more exploration with data to see the interaction among various other variables
```{r}
# table(ojsales_train$MM, ojsales_train$PriceMM)
# prop.table(table(ojsales_train$MM, ojsales_train$PriceMM))
```


You should build at least two classification models to try to predict MM.
Our error metric will be overall accuracy.

## Logistic Regression
# Model 1


```{r}
log.Quality <- glm(PoorCare ~ . - OfficeVisits , data = train, family = 'binomial')
summary(log.Quality)

Call:
glm(formula = PoorCare ~ . - OfficeVisits, family = "binomial", 
    data = train)
```
# 1
```{r}
MM_LogM1 <- glm(MM ~  .,
                    data=ojsales_train, family=binomial(link="logit"))

summary(MM_LogM1)
```
# 2, Remove ListPriceDiff
```{r}
MM_LogM1 <- glm(MM ~  . - ListPriceDiff,
                    data=ojsales_train, family=binomial(link="logit"))

summary(MM_LogM1)
```
# 3, Remove SalePriceMM
```{r}
MM_LogM1 <- glm(MM ~  . - SalePriceMM - ListPriceDiff - SalePriceCH - PriceDiff,
                    data=ojsales_train, family=binomial(link="logit"))

summary(MM_LogM1)
```

Converting fitted values to predictions. We are predicting at 95% confidence interval, hence we are 
```{r}
yhat_LogM1 <- (MM_LogM1$fit > 0.5) * 1
```

Putting the fitted and the predicted values into a dataframe for ease of analysis
```{r}
MM_fit_predictions <- data.frame(MM_LogM1$y, yhat_LogM1)
names(MM_fit_predictions) <- c("yact","yhat_LogM1")
```

## Creating Confusion Matrix
```{r}
table(MM_fit_predictions$yact, MM_fit_predictions$yhat_LogM1)

# Percentage of Precision and Recall 
prop.table(table(MM_fit_predictions$yact, MM_fit_predictions$yhat_LogM1))
```


```{r}
cm_LogM1 <- confusionMatrix(MM_fit_predictions$yhat_LogM1, MM_fit_predictions$yact, 
                            positive = "1")
cm_LogM1
```

Visualizing the rate of purchase of MM vs CH. We got an almost ideal distribution scores with the scores of negative instances (0) to the left represented by the Orange Lines. This Orange solid line represents the distribution of the score for the items known not to be buying CH and not MM. While the dotted green line represents the distribution of the popularity of MM. 
```{r}
library(ggplot2)
ggplot(MM_fit_predictions, aes(x=yhat_LogM1, y=yact)) + geom_point() + 
stat_smooth(method="glm", family="binomial", se=FALSE)

ggplot(ojsales_train, aes(x=yhat_LogM1, color=MM, linetype=MM)) + geom_density()
```


Obviously, `ojsales_train` is your training dataset. After fitting each
model, use the `caret::confusionMatrix` function to create a confusion matrix
for each of the models based on the training data.

You should at least try the following two techniques:
- logistic regression
- a simple decision tree

**HACKER EXTRA:** Try additional techniques such as random forest, k-nearest 
neighbor or others.

Then use the `predict()` function to make classification predictions on the
test dataset and use `caret::confusionMatrix` to create a confusion matrix
for each of the models for the predictions. 

Summarize your results. 
- Which technique performed the best in terms of overall accuracy? 
- Which technique had the best sensitivity score?
- How did accuracy differ for the training and test datasets for each model?
- Is their any evidence of overfitting?